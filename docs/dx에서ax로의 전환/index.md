---
layout: default
title: DX 전환
nav_order: 7
has_children: true
permalink: /docs/dx/
---

# DX 전환

DX에서 AX로의 전환을 위한 AI/ML 교육 과정입니다.

## 커리큘럼

| 주차 | 이론 | 실습 |
|------|------|------|
| 1주차 | AX 시대 (머신러닝 개요) | Python 기초, 머신러닝 |
| 2주차 | 딥러닝 종합 | CNN, 이미지 분류 |
| 3주차 | LLM이란 | LangChain, RAG, 챗봇 |

---

## DX에서 AX로의 전환

### AI의 역사적 맥락

인류는 항상 미래를 예측하고 싶어했다. 최초의 AI라 할 수 있는 것은 **달력**이다. 수학식을 통해 절기(춘분, 추분, 동지 등)를 예측했다.

- 1950년대: 포탄 궤적 계산
- 1980년대: 의사결정시스템
- 2017년: Transformer 등장 → ChatGPT의 기반

### AX 시대의 특징

- **On-Device AI의 부상**: 기기 자체에서 AI 처리
- **전사적 통합**: 기업 전반에 AI 적용
- **효율 극대화**: 반복적이고 규칙적인 작업 자동화

---

## 머신러닝 기초

### 기본 개념

```
y = ax + b

y : 예측하고자 하는 결과 (목표치, 종속변수)
x : y에 영향을 미치는 요인들 (특성, feature, 독립변수)
a : 기울기 (가중치, 회귀계수)
b : 절편 (상수)
```

**예시: 키에 따른 몸무게 예측**
- 가설: "키가 크면 몸무게가 많이 나간다"
- y: 몸무게
- x: 키 (또는 키, 생활습관, 먹는양, 수면양, 유전 등)

### 지도학습의 핵심

a와 b를 구하는 것이 지도학습의 목표다.

**오차 계산**
- 오차(잔차) = 실제값 - 예측값
- 오차를 모두 더하면 양수/음수가 상쇄되어 0이 됨
- 해결책: **제곱법** 사용
  - 분산: 오차의 제곱의 합
  - 표준편차: 분산의 제곱근
- **최소제곱법**: 오차의 제곱의 합이 최소가 되는 지점 찾기

### 경사하강법

오차가 최소가 되는 구간을 찾아내는 방법
- 2차 곡선(포물선)에서 기울기가 0이 되는 지점 탐색
- 기울기 계산에 미분 사용 (Python 함수로 처리 가능)

### 데이터 분석 방식

| 유형 | 설명 | 사용 알고리즘 |
|------|------|---------------|
| 연속형 | 특정 수치 맞추기 | 선형회귀 |
| 비연속형 | 등급/분류 | 로지스틱 회귀 |

> **주의**: 로지스틱 "회귀"라는 이름이지만 실제로는 **분류** 알고리즘이다.

### 과적합 문제

- **과소적합**: 학습 부족
- **과대적합**: 학습 데이터에 너무 맞춰져 새로운 데이터에 대응 못함
- 신경망 레이어를 너무 많이 쌓으면 과대적합 발생

---

## 딥러닝

### 머신러닝 vs 딥러닝

| 구분 | 머신러닝 | 딥러닝 |
|------|----------|--------|
| 특성공학 | 수동 (사람이 직접) | 자동 (컴퓨터가 처리) |
| 주요 알고리즘 | 선형회귀, SVM, 랜덤포레스트 | CNN, RNN, Transformer |

### 주요 모델

| 모델 | 특징 | 용도 |
|------|------|------|
| CNN | 이미지의 고차원 데이터 유지하며 분석 | 이미지 인식의 최고봉 |
| RNN | 시퀀스 처리 | 짧은 문장 분석 |
| Transformer | 롱텀 데이터 분석에 효과적 | LLM의 기반 |

### 딥러닝의 핵심 개념

- **뇌세포 모델링**: 신호가 임계값을 넘으면 자극으로 전달
- **특성추출 자동화**: CNN이 이미지 특성추출의 어려움을 해결
- **풀링(서브샘플링)**: 중요하지 않은 특성 제거

---

## LLM (Large Language Model)

### LLM의 정의

- **L (Large)**: 대규모 - 수십억~수조 개의 매개변수, 수 테라바이트의 학습 데이터
- **L (Language)**: 자연어 텍스트 처리
- **M (Model)**: Transformer 기반 신경망

### 주요 LLM 서비스

- ChatGPT (OpenAI)
- Gemini (Google)
- LLaMA (Meta)
- HuggingFace (오픈소스)

### LLM의 특성

- 과거 데이터 기반이라 **미래 예측에 한계**
- 반복적이고 규칙적인 것은 잘 처리
- 새로운 예외 상황에는 대응 어려움
- C로 구현되어 있음 (Python은 래퍼)

### 파인튜닝

추가 데이터로 파라미터(가중치)를 조정하여 특정 도메인에 맞게 최적화하는 것

---

## 텍스트 처리와 임베딩

### 토큰화와 시퀀스화

```
"I like dog" → [1, 2, 3]
"I like pretty dog" → [1, 2, 4, 3]
```

### 임베딩의 필요성

희소행렬(Sparse Matrix) 문제 해결
- 단어 하나 저장에 수억 바이트 필요 → 비효율적
- 해결책: 단어 간 거리(의미적 유사도)를 벡터로 표현

### 임베딩의 장점

- 고차원 데이터를 저차원 밀집 벡터로 변환 → 계산 효율성 향상
- 코사인 유사도로 의미적으로 유사한 데이터 탐색
- 머신러닝 모델 성능 향상

### 활용 예시

- 질문 이해
- 감정 분석
- 기계 번역

---

## Python 라이브러리

| 라이브러리 | 특징 |
|------------|------|
| NumPy | 배열의 벡터 연산 특화, 같은 타입 데이터만 |
| Pandas | 여러 타입 혼합 가능, DataFrame 사용 |
| Keras | 텐서플로우에 내장, 쉬운 사용 (가구로 비유하면 일룸) |
| PyTorch | LLM 개발자들이 주로 사용 (가구로 비유하면 IKEA) |

---

## AI의 산업 적용

### AI로 인해 변화가 예상되는 직업

- 번역
- 콜센터
- 단순 사무직
- 동시통역

### AI 적용 사례

**의료 진단 예시**
- 진단1: 100명 중 암환자 10명 포함 20명 선별
- 진단2: 100명 중 암환자 10명 중 8명만 선별, 2명 누락

→ 진단1이 더 우수. 20명에서 정밀검사하면 되므로 누락보다 낫다.

### 데이터 분석 사례

**월마트 장바구니 분석**: 맥주와 기저귀의 연관성 발견

---

## 보안 이슈

### 프롬프트 인젝션

흰색 폰트를 이용해 이력서나 논문에 긍정적 평가 문자열을 숨기는 공격

### 프롬프트 탈옥

LLM의 안전 장치를 우회하는 기법

### LLM의 훈련데이타의 최종날짜 알아내기.

---

## 참고 자료

- [실습 자료](https://url.kr/istnob)
- [Google Drive](https://drive.google.com/drive/folders/1pk8w12xmNxSUm5mx6zB19AtvsI52gOpn)
- [직업훈련교사 자격증](https://hrdi.koreatech.ac.kr/)
- [Google AI Studio](https://aistudio.google.com/)
