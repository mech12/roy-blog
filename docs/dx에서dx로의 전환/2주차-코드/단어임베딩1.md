---
layout: default
title: 단어임베딩1.py
parent: 2주차 코드
grand_parent: DX 전환
nav_order: 12
---

# 단어임베딩1.py

IMDB 리뷰 데이터를 활용한 단어 임베딩 학습

## 핵심 개념
- Embedding 레이어 (단어 → 벡터)
- IMDB 영화 리뷰 데이터셋
- pad_sequences를 통한 시퀀스 정규화
- 이진 분류 (긍정/부정)

```python
"""
단어임베딩 : 단어로부터의 학습이다.
             256, 512, 1024 차원의 단어 임베딩을 사용한다

학습하거나, 이미 학습된 내용을 가져온다
"""

from tensorflow.keras.datasets import imdb
from tensorflow.keras import preprocessing

# 특성으로 사용할 단어의 수
max_features = 10000
# 사용할 텍스트의 길이(가장 빈번한 max_features 개의 단어만 사용합니다)
maxlen = 20
#리뷰에서 20개의 단어만 사용하려고 함

# 정수 리스트로 데이터를 로드합니다.- 문장을 시퀀스로 전환
# 시퀀스를 필요로 한다  - 케라스의 토큰나이저 사용
(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)

print(x_train[:1])

# list ->  (samples, maxlen) 크기의 2D 정수 텐서로 변환합니다.
#문장 전체가 아니라 뒤의 20단어만 사용한다
#앞의 20단어를 원하면 truncating='post' 붙이면 된다.
#보통은 문장의 끝에 결론이 오기때문에 뒷문장을 사용한다
x_train = preprocessing.sequence.pad_sequences(x_train, maxlen=maxlen)
x_test = preprocessing.sequence.pad_sequences(x_test, maxlen=maxlen)

print("padding후")
print(x_train[:1])


from keras.models import Sequential
from keras.layers import Flatten, Dense, Embedding

model = Sequential()
# 나중에 임베딩된 입력을 Flatten 층에서 펼치기 위해 Embedding 층에 input_length를 지정합니다.
# 임베딩 공간의 크기 : 64차원
#Embedding 층이 처음에 와야 한다
model.add(Embedding(10000, 64, input_length=maxlen))

#input_dim: 정수 > 0. 어휘목록의 크기, 최대 정수 색인 + 1. 사용하는 단어의 크기
#output_dim: 정수 >= 0. 밀집 임베딩의 차원.
#input_length: 인풋 시퀀스의 길이로, 그 길이가 불변해야 한다

#학습을 시작한다
model.add(Flatten()) #3D임베딩 텐서를 samples, maxlength*8 크기의 2D텐서로 펼친다
# 이진분류
model.add(Dense(1, activation='sigmoid'))
model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])
model.summary()

history = model.fit(x_train, y_train,
                    epochs=10,
                    batch_size=32,
                    validation_split=0.2)

#평가하기
results = model.evaluate(x_test, y_test)
print(results)
```

## 모델 구조

```
입력 (20개 단어 인덱스)
    ↓
Embedding(10000, 64)  → 20x64 행렬
    ↓
Flatten()  → 1280차원 벡터
    ↓
Dense(1, sigmoid)  → 0~1 확률
```
