#conda activate llm_env
#pip install openai


MYKEY="YOUR_OPENAI_API_KEY"

from openai import OpenAI
# Image ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì–´ ìˆë‹¤ë©´, ì´ë¯¸ì§€ë¥¼ ë°”ë¡œ ì—´ì–´ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
# from PIL import Image
# import requests
# from io import BytesIO

# í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
client = OpenAI(api_key=MYKEY)

from openai import OpenAI
import json

# í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
client = OpenAI()

# ğŸ’¡ ëª¨ë¸ì´ í˜¸ì¶œí•  ìˆ˜ ìˆëŠ” í•¨ìˆ˜ë¥¼ ì •ì˜í•©ë‹ˆë‹¤.
def get_current_weather(city: str, unit: str = "celsius") -> str:
    """ì£¼ì–´ì§„ ë„ì‹œì˜ í˜„ì¬ ë‚ ì”¨ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤."""
    print(f"[Function Called: get_current_weather(city='{city}', unit='{unit}')]")
    
    # ì‹¤ì œ API í˜¸ì¶œ ëŒ€ì‹  ê°€ìƒì˜ ë°ì´í„°ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    if "seoul" in city.lower():
        return json.dumps({"city": "Seoul", "temperature": "15", "unit": unit, "forecast": "sunny"})
    elif "busan" in city.lower():
        return json.dumps({"city": "Busan", "temperature": "18", "unit": unit, "forecast": "cloudy"})
    else:
        return json.dumps({"city": city, "temperature": "22", "unit": unit, "forecast": "light rain"})

# ğŸ’¡ í•¨ìˆ˜ ëª©ë¡ì„ ë”•ì…”ë„ˆë¦¬ë¡œ ì •ì˜í•©ë‹ˆë‹¤. (í•¨ìˆ˜ ì´ë¦„: ì‹¤ì œ í•¨ìˆ˜)
available_functions = {
    "get_current_weather": get_current_weather,
}

def run_function_calling_example():
    """í•¨ìˆ˜ í˜¸ì¶œ(Tool Use) ì˜ˆì œ ì‹¤í–‰"""
    print("--- 3. í•¨ìˆ˜ í˜¸ì¶œ ì˜ˆì œ ì‹œì‘ ---")

    # 1. ëª¨ë¸ì—ê²Œ ì „ë‹¬í•  í•¨ìˆ˜ ì •ì˜ (JSON Schema í˜•ì‹)
    tools = [
        {
            "type": "function",
            "function": {
                "name": "get_current_weather",
                "description": "íŠ¹ì • ë„ì‹œì˜ í˜„ì¬ ë‚ ì”¨ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.",
                "parameters": {
                    "type": "object",
                    "properties": {
                        "city": {
                            "type": "string",
                            "description": "ë‚ ì”¨ë¥¼ ì•Œê³  ì‹¶ì€ ë„ì‹œ ì´ë¦„ (ì˜ˆ: ì„œìš¸, ë¶€ì‚°)"
                        },
                        "unit": {
                            "type": "string",
                            "enum": ["celsius", "fahrenheit"],
                            "description": "ì˜¨ë„ ë‹¨ìœ„. ê¸°ë³¸ê°’ì€ ì„­ì”¨(celsius)"
                        },
                    },
                    "required": ["city"],
                },
            },
        }
    ]

    # 2. ì‚¬ìš©ì ì§ˆë¬¸
    messages = [{"role": "user", "content": "ì„œìš¸ì˜ í˜„ì¬ ë‚ ì”¨ëŠ” ì–´ë•Œ? ë‹¨ìœ„ëŠ” ì„­ì”¨ë¡œ ì•Œë ¤ì¤˜."}]

    # 3. ëª¨ë¸ì—ê²Œ ìš”ì²­ (í•¨ìˆ˜ ì •ì˜ í¬í•¨)
    response = client.chat.completions.create(
        model="gpt-3.5-turbo",
        messages=messages,
        tools=tools,
        tool_choice="auto", # ëª¨ë¸ì´ í•¨ìˆ˜ í˜¸ì¶œ ì—¬ë¶€ë¥¼ ê²°ì •í•˜ë„ë¡ í•¨
    )

    response_message = response.choices[0].message
    
    # 4. ëª¨ë¸ì´ í•¨ìˆ˜ í˜¸ì¶œì„ ìš”ì²­í–ˆëŠ”ì§€ í™•ì¸
    if response_message.tool_calls:
        print("\nğŸ¤– AI ì‘ë‹µ: í•¨ìˆ˜ í˜¸ì¶œ ìš”ì²­ì„ ë°›ì•˜ìŠµë‹ˆë‹¤.")
        
        # í•¨ìˆ˜ í˜¸ì¶œ ìƒì„¸ ì •ë³´ ì¶œë ¥
        tool_calls = response_message.tool_calls
        messages.append(response_message) # ëª¨ë¸ì˜ ìš”ì²­ì„ ëŒ€í™” ê¸°ë¡ì— ì¶”ê°€
        
        # í•¨ìˆ˜ í˜¸ì¶œ ì‹¤í–‰
        for tool_call in tool_calls:
            function_name = tool_call.function.name
            function_to_call = available_functions[function_name]
            function_args = json.loads(tool_call.function.arguments)

            print(f"  - í˜¸ì¶œí•  í•¨ìˆ˜: {function_name}")
            print(f"  - ì „ë‹¬ ì¸ì: {function_args}")
            
            # ì‹¤ì œ í•¨ìˆ˜ ì‹¤í–‰ ë° ê²°ê³¼ ë°˜í™˜
            function_response = function_to_call(
                city=function_args.get("city"),
                unit=function_args.get("unit")
            )

            # 5. í•¨ìˆ˜ì˜ ì‹¤í–‰ ê²°ê³¼ë¥¼ ëª¨ë¸ì—ê²Œ ë‹¤ì‹œ ì „ë‹¬
            messages.append(
                {
                    "tool_call_id": tool_call.id,
                    "role": "tool",
                    "name": function_name,
                    "content": function_response, # í•¨ìˆ˜ì˜ JSON ê²°ê³¼
                }
            )

        # 6. ìµœì¢… ë‹µë³€ ìš”ì²­
        final_response = client.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=messages, # ì´ì „ ëª¨ë“  ê¸°ë¡ (ìš”ì²­, ì‘ë‹µ, í•¨ìˆ˜ ê²°ê³¼) í¬í•¨
        )

        final_answer = final_response.choices[0].message.content
        print(f"\nâœ… ìµœì¢… AI ë‹µë³€: {final_answer}")

    else:
        # ëª¨ë¸ì´ í•¨ìˆ˜ í˜¸ì¶œ ì—†ì´ ë°”ë¡œ ë‹µë³€í•œ ê²½ìš°
        print("ğŸ¤– AI ì‘ë‹µ: í•¨ìˆ˜ í˜¸ì¶œ ì—†ì´ ë°”ë¡œ ë‹µë³€í•©ë‹ˆë‹¤.")
        print(response_message.content)

    print("--- í•¨ìˆ˜ í˜¸ì¶œ ì˜ˆì œ ì¢…ë£Œ ---\n")

# run_function_calling_example()