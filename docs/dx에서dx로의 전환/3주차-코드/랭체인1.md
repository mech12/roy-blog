---
layout: default
title: ë­ì²´ì¸1.py
parent: 3ì£¼ì°¨ ì½”ë“œ
grand_parent: 3ì£¼ì°¨ - LLM
nav_order: 1
---

# ë­ì²´ì¸1.py

LangChain + Google Geminië¥¼ í™œìš©í•œ ê¸°ë³¸ LLM í˜¸ì¶œ

---

## ê°œìš”

| í•­ëª© | ë‚´ìš© |
|-----|------|
| **í”„ë ˆì„ì›Œí¬** | LangChain |
| **LLM** | Google Gemini 2.5 Flash |
| **í•µì‹¬ ê°œë…** | LCEL (LangChain Expression Language) |

---

## ì „ì²´ ì†ŒìŠ¤ ì½”ë“œ

```python
# â­ï¸ 1. API í‚¤ ì„¤ì •
MyKey = "YOUR_GEMINI_API_KEY"  # ì—¬ê¸°ì— ì‹¤ì œ í‚¤ë¥¼ ë„£ì–´ ì‚¬ìš©í•˜ì„¸ìš”.

import os
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from getpass import getpass

# â­ï¸ API í‚¤ ì„¤ì • (í‚¤ë¥¼ ì§ì ‘ ì…ë ¥í•˜ê±°ë‚˜ í™˜ê²½ ë³€ìˆ˜ì— ì„¤ì •)
if "GEMINI_API_KEY" not in os.environ:
    os.environ["GEMINI_API_KEY"] = MyKey

# 1. ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
llm = ChatGoogleGenerativeAI(model="gemini-2.5-flash")

# 2. í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì •ì˜
prompt = ChatPromptTemplate.from_messages([
    ("system", "ë‹¹ì‹ ì€ ì‚¬ìš©ìì—ê²Œ ì¹œì ˆí•˜ê³  ìœ ë¨¸ëŸ¬ìŠ¤í•˜ê²Œ ë‹µë³€í•˜ëŠ” ì±—ë´‡ì…ë‹ˆë‹¤."),
    ("user", "{input}")
])

# 3. LCEL ì²´ì¸ êµ¬ì¶•: í”„ë¡¬í”„íŠ¸ | LLM | íŒŒì„œ
# ì´ íŒŒì´í”„ë¼ì¸ì´ LLMChainì„ ëŒ€ì²´í•©ë‹ˆë‹¤.
basic_chain = prompt | llm | StrOutputParser()

# 4. ì²´ì¸ ì‹¤í–‰
question = "ì¸ê³µì§€ëŠ¥ì´ë€ ë¬´ì—‡ì¸ì§€ ì•„ì£¼ ì§§ê²Œ ì„¤ëª…í•´ì¤˜."
response = basic_chain.invoke({"input": question})

print("--- ê¸°ë³¸ LLM í˜¸ì¶œ ê²°ê³¼ ---")
print(response)
```

---

## ë‹¨ê³„ë³„ ì„¤ëª…

### Step 1: ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸

```python
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
```

| ëª¨ë“ˆ | ìš©ë„ |
|-----|------|
| `ChatGoogleGenerativeAI` | Google Gemini ëª¨ë¸ ë˜í¼ |
| `ChatPromptTemplate` | í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì •ì˜ |
| `StrOutputParser` | LLM ì¶œë ¥ì„ ë¬¸ìì—´ë¡œ íŒŒì‹± |

---

### Step 2: í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿

```python
prompt = ChatPromptTemplate.from_messages([
    ("system", "ë‹¹ì‹ ì€ ì¹œì ˆí•œ ì±—ë´‡ì…ë‹ˆë‹¤."),  # ì‹œìŠ¤í…œ ë©”ì‹œì§€
    ("user", "{input}")                        # ì‚¬ìš©ì ì…ë ¥ (ë³€ìˆ˜)
])
```

**ì—­í•  (Role) ì¢…ë¥˜:**

| Role | ì„¤ëª… |
|------|------|
| `system` | AIì˜ ì„±ê²©ê³¼ í–‰ë™ ë°©ì‹ ì •ì˜ |
| `user` | ì‚¬ìš©ìì˜ ì§ˆë¬¸ |
| `assistant` | AIì˜ ì´ì „ ì‘ë‹µ (ëŒ€í™” ê¸°ë¡ìš©) |

---

### Step 3: LCEL ì²´ì¸ êµ¬ì¶•

```python
basic_chain = prompt | llm | StrOutputParser()
```

**LCEL (LangChain Expression Language)**

íŒŒì´í”„ ì—°ì‚°ì `|`ë¥¼ ì‚¬ìš©í•´ ì»´í¬ë„ŒíŠ¸ë¥¼ ì—°ê²°:

```
í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ â†’ LLM í˜¸ì¶œ â†’ ì¶œë ¥ íŒŒì‹±
     prompt    â†’    llm    â†’ StrOutputParser()
```

---

### Step 4: ì‹¤í–‰

```python
response = basic_chain.invoke({"input": "ì§ˆë¬¸ ë‚´ìš©"})
```

`invoke()`ì— ë”•ì…”ë„ˆë¦¬ë¡œ ë³€ìˆ˜ ì „ë‹¬ â†’ ìµœì¢… ë¬¸ìì—´ ì‘ë‹µ ë°˜í™˜

---

## ì‹¤í–‰ ë°©ë²•

```bash
# ê°€ìƒí™˜ê²½ í™œì„±í™”
conda activate myenv

# í•„ìš” ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜
pip install langchain langchain-google-genai google-generativeai

# ì‹¤í–‰
python ë­ì²´ì¸1.py
```

---

## ì¶œë ¥ ì˜ˆì‹œ

```
--- ê¸°ë³¸ LLM í˜¸ì¶œ ê²°ê³¼ ---
ì¸ê³µì§€ëŠ¥ì€ ì»´í“¨í„°ê°€ ì‚¬ëŒì²˜ëŸ¼ ìƒê°í•˜ê³  í•™ìŠµí•˜ëŠ” ê¸°ìˆ ì´ì•¼!
ë§ˆì¹˜ ë¡œë´‡ ì¹œêµ¬ê°€ ê³µë¶€í•´ì„œ ë˜‘ë˜‘í•´ì§€ëŠ” ê²ƒê³¼ ê°™ì•„~ ğŸ¤–
```
